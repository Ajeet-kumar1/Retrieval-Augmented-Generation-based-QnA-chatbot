{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d50d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def inference(url, prompt):\n",
    "    payload = {\n",
    "        \"model\": 'deepseek-r1:1.5b',\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 2\n",
    "    \n",
    "    }\n",
    "    # Send the request\n",
    "    response = requests.post(url, json=payload)\n",
    "    final_response = \"\"\n",
    "    for chunk in response.text.split(\"\\n\"):\n",
    "        if chunk.strip():  # Skip empty lines\n",
    "            data = json.loads(chunk)\n",
    "            final_response += data.get(\"response\", \"\")\n",
    "    return final_response\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Define the request parameters\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    t = True\n",
    "    while t==True:\n",
    "        prompt = input(\"Enter your query here: \")\n",
    "        if prompt==\"exit\":\n",
    "            t = False\n",
    "        else:\n",
    "            final_response = inference(url, prompt)\n",
    "            print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f0c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' or 'quit' to stop the chatbot.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def inference(url, prompt):\n",
    "    payload = {\n",
    "        \"model\": 'deepseek-r1:1.5b',\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 100  # Increase if you want longer responses\n",
    "    }\n",
    "    # Send the request\n",
    "    response = requests.post(url, json=payload)\n",
    "    final_response = \"\"\n",
    "    for chunk in response.text.split(\"\\n\"):\n",
    "        if chunk.strip():  # Skip empty lines\n",
    "            data = json.loads(chunk)\n",
    "            final_response += data.get(\"response\", \"\")\n",
    "    return final_response\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    print(\"Type 'exit' or 'quit' to stop the chatbot.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        prompt = input(\"You: \")\n",
    "        if prompt.strip().lower() in ['exit', 'quit']:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        try:\n",
    "            final_response = inference(url, prompt)\n",
    "            print(\"Chatbot:\", final_response)\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b86d0509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Document: RAG is very popular in making chatbot\n",
      "Llama Response: \\u003cthink\\u003e\\nOkay, so I need to figure out what RAG stands for and explain it. From the query, I remember that someone asked about RAG being very popular in making chatbots, and now I'm supposed to provide a detailed explanation.\\n\\nFirst, I should start by understanding what a chatbot is. A chatbot is an automated system designed to interact with a user through text or voice messages. It can generate responses, answer questions, and provide information on various topics.\\n\\nNow, RAG seems to be associated with chatbots but sounds more like something specific. The query mentions it's very popular in making chatbots, so I'm thinking of its name because of the \\A\\ at the end, which makes me wonder if it stands for something related to AI or human-computer interaction.\\n\\nWait, RAGâ€”maybe it's an acronym? Let me think... RG is a common abbreviation. Maybe A is another part. If I break it down: R is robot, G could be generation, A is answer, so maybe it's \\Ask Generation.\\ That doesn't make sense.\\n\\nAlternatively, maybe it's \\Robot Generation,\\ but that seems too generic. Perhaps there's more to it. Another thought: \\RAG\\ might stand for \\Robotic Assistant Generation,\\ but I'm not sure if that's correct.\\n\\nI recall that some chatbots are called RAGs because they can generate responses by understanding the user's query. The name suggests a generation of responses, maybe from different experts or models. That could be it. So, RAG would mean \\Rational Assistant Generation,\\ implying that each RAG is built based on the knowledge and expertise of an AI system.\\n\\nI should verify this quickly. If I search for RAG chatbot, I might find that RAG stands for Rational Assistant, which is a model designed to handle various tasks like answering questions, providing information, and even generating creative content. It's known for being versatile and capable across different domains.\\n\\nSo, putting it all together: RAG is an AI-powered chatbot that generates responses by understanding the user's intent. The name implies a level of rationality or intelligence in its responses, making it very popular among developers and researchers looking to create efficient chatbots.\\n\\u003c/think\\u003e\\n\\nRAG stands for Rational Assistant, an AI-powered chatbot designed to generate responses based on natural language understanding. It is known for its versatility and ability to handle various tasks across different domains, making it a popular choice for developers and researchers looking to build efficient chatbots. The name suggests a level of rationality in its responses, enhancing its capability to provide comprehensive and coherent answers.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import re\n",
    "# Step 1: Load documents (example corpus)\n",
    "documents = [\n",
    "    \"Python is a popular programming language for AI.\",\n",
    "    \"Threading in Python allows concurrent execution of code.\",\n",
    "    \"Llama 3.2 is a state-of-the-art language model by Ollama.\",\n",
    "    \"RAG stands for Retrieval-Augmented Generation.\",\n",
    "    \"RAG is very popular in making chatbot\"\n",
    "  \n",
    "]\n",
    "\n",
    "# Step 2: Convert documents to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Function to retrieve relevant documents\n",
    "def retrieve_docs(query):\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vector, doc_vectors)\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    return documents[most_similar_idx]\n",
    "\n",
    "# Step 3: Query the RAG system\n",
    "def rag_query(query):\n",
    "    relevant_doc = retrieve_docs(query)\n",
    "    print(f\"Retrieved Document: {relevant_doc}\")\n",
    "    \n",
    "    # Step 4: Send the relevant document and query to Ollama API (using Llama 3.2)\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": 'deepseek-r1:1.5b', \"prompt\": f\"{relevant_doc}\\n\\nQuery: {query}\"}\n",
    "    )\n",
    "   \n",
    "      # Handle the raw response text\n",
    "        # Handle the raw response text\n",
    "    raw_response = response.text\n",
    "     # Extract all 'response' values from the JSON objects\n",
    "    responses = re.findall(r'\"response\":\\s*\"([^\"]+)\"', raw_response)\n",
    "    # Combine all parts into a complete sentence\n",
    "    final_response = \"\".join(responses)\n",
    "    print(f\"Llama Response: {final_response}\")\n",
    "\n",
    "# Example Query\n",
    "rag_query(\"What is RAG?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66bd643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ajeet\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'faiss' has no attribute 'IndexFlatL2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 87\u001b[0m\n\u001b[0;32m     83\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAjeet\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mattention-is-all-you-need.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType the query: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "Cell \u001b[1;32mIn[1], line 72\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(pdf_path, query)\u001b[0m\n\u001b[0;32m     69\u001b[0m split_docs \u001b[38;5;241m=\u001b[39m split_documents(pages)   \u001b[38;5;66;03m# Split properly\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Create vector store\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m index, document_texts, embedder \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Retrieve context\u001b[39;00m\n\u001b[0;32m     75\u001b[0m context \u001b[38;5;241m=\u001b[39m retrieve_context(query, embedder, index, document_texts)\n",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m, in \u001b[0;36mcreate_vector_store\u001b[1;34m(split_docs)\u001b[0m\n\u001b[0;32m     29\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedder\u001b[38;5;241m.\u001b[39mencode(document_texts)\n\u001b[0;32m     31\u001b[0m dimension \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 32\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mfaiss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexFlatL2\u001b[49m(dimension)\n\u001b[0;32m     33\u001b[0m index\u001b[38;5;241m.\u001b[39madd(embeddings\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, document_texts, embedder\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'faiss' has no attribute 'IndexFlatL2'"
     ]
    }
   ],
   "source": [
    "# 1. Install required packages\n",
    "# pip install pypdf langchain sentence-transformers faiss-cpu ollama\n",
    "\n",
    "import ollama\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.schema import Document  # Add this import\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 2. Load and process PDF document\n",
    "def load_pdf_documents(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    return loader.load()  # Returns list of Document objects\n",
    "\n",
    "# 3. Split text into chunks\n",
    "def split_documents(pages):  # Changed parameter name\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    return text_splitter.split_documents(pages)  # Directly split Document objects\n",
    "\n",
    "# 4. Create vector store\n",
    "def create_vector_store(split_docs):  # Changed parameter name\n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    document_texts = [doc.page_content for doc in split_docs]  # Use .page_content\n",
    "    embeddings = embedder.encode(document_texts)\n",
    "    \n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    return index, document_texts, embedder\n",
    "\n",
    "# 5. Retrieve relevant context (unchanged)\n",
    "def retrieve_context(query, embedder, index, documents, k=3):\n",
    "    query_embedding = embedder.encode([query])\n",
    "    distances, indices = index.search(query_embedding.astype(np.float32), k)\n",
    "    return [documents[i] for i in indices[0]]\n",
    "\n",
    "# 6. Generate answer using Ollama (unchanged)\n",
    "def generate_answer_with_ollama(query, context):\n",
    "    formatted_context = \"\\n\".join(context)\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert assistant trained on document information.\n",
    "    Use this context to answer the question:\n",
    "    \n",
    "    {formatted_context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer in detail using only the provided context:\"\"\"\n",
    "    \n",
    "    response = ollama.generate(\n",
    "        model='deepseek-r1:1.5b',\n",
    "        prompt= prompt,\n",
    "        options={\n",
    "            'temperature': 0.3,\n",
    "            'max_tokens': 2000\n",
    "        }\n",
    "    )\n",
    "    return response['response']\n",
    "\n",
    "# Main workflow (modified)\n",
    "def main(pdf_path, query):\n",
    "    # Load and process PDF\n",
    "    pages = load_pdf_documents(pdf_path)  # Get Document objects\n",
    "    split_docs = split_documents(pages)   # Split properly\n",
    "    \n",
    "    # Create vector store\n",
    "    index, document_texts, embedder = create_vector_store(split_docs)\n",
    "    \n",
    "    # Retrieve context\n",
    "    context = retrieve_context(query, embedder, index, document_texts)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = generate_answer_with_ollama(query, context)\n",
    "    return answer\n",
    "\n",
    "# Example usage (unchanged)\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\Ajeet\\Downloads\\attention-is-all-you-need.pdf\"\n",
    "\n",
    "    query = input(\"Type the query: \")\n",
    "    \n",
    "    result = main(pdf_path, query)\n",
    "    print(\"Answer:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed59461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
